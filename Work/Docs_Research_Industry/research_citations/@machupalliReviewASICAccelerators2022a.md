---
id: machupalliReviewASICAccelerators2022a
type: article-journal
title: Review of ASIC accelerators for deep neural network
issued:
  date-parts:
    - - 2022
      - 3
page: "104441"
URL: https://www.sciencedirect.com/science/article/pii/S0141933122000163
DOI: 10.1016/j.micpro.2022.104441
container-title: Microprocessors and Microsystems
volume: 89
abstract: Deep neural networks (DNNs) have become an essential tool in artificial intelligence, with a wide range of applications such as computer vision, medical diagnosis, security, robotics, and autonomous vehicle. The DNNs deliver the state-of-the-art performance in many applications. The complexity of the DNN models generally increases with application complexity and deployment of complex DNN models requires high computational power. General-purpose processors are unable to process complex DNNs within the required throughput, latency, and power budget. Therefore, domain-specific hardware accelerators are required to provide high computational resources with superior energy efficiency and throughput within a small chip area. In this paper, existing DNN hardware accelerators are reviewed and classified based on the optimization techniques used in their implementations. Each optimization technique generally improves one or more specific performance parameter(s). For example, the hardware optimized for sparse DNNs may provide poor performance for dense DNNs in terms of power and throughput. Therefore, understanding the tradeoff between different hardware accelerators helps to identify the best accelerator model for application deployment. We identify three major areas, ALU, dataflow, and sparsity, in hardware architectures having the potential to improve the overall performance of an accelerator. Existing hardware accelerators for inference are broadly classified into these three categories. As there is no standard model or performance metrics to evaluate the efficiency of the new DNN hardwares in the literature, the classification model can help to identify appropriate performance parameters and benchmark accelerators.
tags:
  - literature_note
author:
  - family: Machupalli
    given: Raju
    literal: ""
  - family: Hossain
    given: Masum
    literal: ""
  - family: Mandal
    given: Mrinal
    literal: ""
accessed:
  date-parts:
    - - 2023
      - 9
      - 1
citation-label: machupalliReviewASICAccelerators2022a
ISSN: 0141-9331
keyword: computation/deep_learning/application
year: "2022"
dateCreated: 2025-05-25
reading-status: to-read
aliases:
  - Review of ASIC accelerators for deep neural network
author-links:
  - "[[Author/Raju Machupalli]]"
  - "[[Author/Masum Hossain]]"
  - "[[Author/Mrinal Mandal]]"
attachment: []
related:
  - []
---

# Review of ASIC accelerators for deep neural network

## Summary
summary::
rating::

## Quotes

## Notes

## Figures

## References

ðŸ”— [Source](https://www.sciencedirect.com/science/article/pii/S0141933122000163)

